import io
import re
from typing import List, Tuple

import streamlit as st
from pypdf import PdfReader
from langdetect import detect, DetectorFactory
from deep_translator import GoogleTranslator

# Make langdetect deterministic
DetectorFactory.seed = 0


def extract_text_from_pdf(file_bytes: bytes) -> str:
	"""Extract text from a PDF file given as bytes using pypdf.

	Returns an empty string if no text is found or on failure.
	"""
	try:
		reader = PdfReader(io.BytesIO(file_bytes))
		texts: List[str] = []
		for page in reader.pages:
			try:
				texts.append(page.extract_text() or "")
			except Exception:
				# Skip pages that fail to extract
				continue
		return "\n".join(t.strip() for t in texts if t)
	except Exception:
		return ""


def split_sentences(text: str) -> List[str]:
	"""Split text into sentences for both English and Korean heuristically."""
	if not text:
		return []

	# Normalize whitespace
	text = re.sub(r"\s+", " ", text).strip()

	# Heuristic: split on end punctuation for English/Korean/Chinese
	pattern = r"(?<=[\.!?ã€‚ï¼ï¼Ÿ])\s+"
	sentences = re.split(pattern, text)

	# Additionally break on frequent Korean sentence endings like 'ë‹¤.' if not already caught
	refined: List[str] = []
	for s in sentences:
		parts = re.split(r"(?<=ë‹¤\.)\s+", s)
		refined.extend(parts)

	# Cleanup
	refined = [s.strip() for s in refined if s and len(s.strip()) > 1]
	return refined


def tokenize_words(text: str) -> List[str]:
	"""Very light word tokenizer that works for Latin scripts and approximates for CJK/Korean.

	For Korean, we approximate by splitting on spaces and punctuation (no morphological analysis).
	"""
	if not text:
		return []
	# Lowercase for English-like languages; keep others as-is
	lowered = text.lower()
	# Replace punctuation with spaces
	normalized = re.sub(r"[^\wê°€-í£ä¸€-é¾¯ã-ã‚”ã‚¡-ãƒ´ãƒ¼ã€…ã€†ã€¤]+", " ", lowered)
	# Split on whitespace
	return [w for w in normalized.split() if w]


def get_stopwords() -> set:
	# Minimal English and Korean stopwords to avoid extra dependencies
	english = {
		"the","is","in","at","of","and","a","to","for","on","with","as","by","an","be","or","it","that","this","from","are","was","were","but","not","have","has","had","we","you","they","their","our","your","can","will","would","should","could","about",
	}
	korean = {
		"ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ë˜í•œ","ë°","ë“±","ì´ëŠ”","ê·¸","ì´","ì €","ê²ƒ","ìˆ˜","ìˆëŠ”","ìˆëŠ”ì§€","ìˆë‹¤","í•˜ì˜€ë‹¤","í–ˆë‹¤","ìœ¼ë¡œ","ì—ì„œ","ì—ê²Œ","ê¹Œì§€","ë¶€í„°","ë³´ë‹¤","ë˜","ë˜ëŠ”","ë•Œë¬¸ì—","ì¦‰","ë˜í•œ","ê·¸ëŸ¬ë¯€ë¡œ","ë”°ë¼ì„œ","í•˜ë©°","í•˜ë©°","í•˜ë©°","ìš°ë¦¬","ë„ˆ","ì—¬ëŸ¬","ë“±ì˜"," ë“±ì˜",
	}
	return set(list(english) + list(korean))


def build_word_frequencies(sentences: List[str]) -> dict:
	stopwords = get_stopwords()
	frequencies: dict = {}
	for sentence in sentences:
		for word in tokenize_words(sentence):
			if word in stopwords:
				continue
			frequencies[word] = frequencies.get(word, 0) + 1
	# Normalize frequencies
	if not frequencies:
		return {}
	max_freq = max(frequencies.values())
	for word in list(frequencies.keys()):
		frequencies[word] = frequencies[word] / max_freq
	return frequencies


def score_sentences(sentences: List[str], word_freq: dict) -> List[Tuple[str, float, int]]:
	"""Return list of (sentence, score, index)."""
	scored: List[Tuple[str, float, int]] = []
	for idx, sentence in enumerate(sentences):
		if not sentence.strip():
			continue
		words = tokenize_words(sentence)
		if not words:
			continue
		score = sum(word_freq.get(w, 0.0) for w in words) / (len(words) + 1e-6)
		scored.append((sentence, score, idx))
	return scored


def summarize_text(text: str, max_sentences: int = 5) -> str:
	"""Simple extractive summarization by word-frequency scoring.

	- Scores sentences by average of normalized word frequencies
	- Selects top-K sentences while preserving original order
	"""
	sentences = split_sentences(text)
	if not sentences:
		return ""
	word_freq = build_word_frequencies(sentences)
	if not word_freq:
		# Fallback: return first K sentences
		return " " .join(sentences[:max(1, max_sentences)])
	# Score and pick top K
	scored = score_sentences(sentences, word_freq)
	if not scored:
		return " " .join(sentences[:max(1, max_sentences)])
	# Sort by score desc, then take top K, and reorder by original index
	top_k = sorted(sorted(scored, key=lambda x: x[1], reverse=True)[:max(1, max_sentences)], key=lambda x: x[2])
	ordered_sentences = [s for s, _, _ in top_k]
	return " " .join(ordered_sentences)


def format_stats(text: str) -> Tuple[int, int, int]:
	chars = len(text)
	words = len(tokenize_words(text))
	sents = len(split_sentences(text))
	return chars, words, sents


def detect_language(text: str) -> str:
	"""Detect language code like 'en', 'ko'. Returns 'unknown' on failure."""
	try:
		if not text or len(text.strip()) < 20:
			return "unknown"
		return detect(text[:5000])
	except Exception:
		return "unknown"


def chunk_text(text: str, max_chars: int = 4500) -> List[str]:
	chunks: List[str] = []
	start = 0
	length = len(text)
	while start < length:
		end = min(start + max_chars, length)
		# try to break on whitespace for nicer chunks
		if end < length:
			ws = text.rfind(" ", start, end)
			if ws != -1 and ws - start > 500:  # avoid tiny tail
				end = ws
		chunks.append(text[start:end])
		start = end
	return chunks


def translate_to_korean(text: str) -> str:
	"""Translate long English text to Korean by chunking."""
	translator = GoogleTranslator(source="auto", target="ko")
	parts = chunk_text(text)
	translated_parts: List[str] = []
	for part in parts:
		try:
			translated_parts.append(translator.translate(part))
		except Exception:
			# If one chunk fails, keep original to avoid data loss
			translated_parts.append(part)
	return "\n".join(translated_parts)


def main() -> None:
	st.set_page_config(page_title="PDF ìš”ì•½ê¸°", page_icon="ğŸ“", layout="centered")
	st.title("ğŸ“ PDF ìš”ì•½ê¸° (Streamlit)")
	st.write("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ê°„ë‹¨íˆ ìš”ì•½í•´ ë“œë¦½ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì´ë‚˜ ëŒ€í˜• ëª¨ë¸ ì—†ì´ ë¡œì»¬ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤.")

	with st.sidebar:
		st.header("ì„¤ì •")
		max_sentences = st.slider("ìš”ì•½ ë¬¸ì¥ ìˆ˜", min_value=2, max_value=15, value=5, step=1)
		st.caption("ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜ ì¶”ì¶œ ìš”ì•½ì…ë‹ˆë‹¤. ì›ë¬¸ í’ˆì§ˆê³¼ ì–¸ì–´ì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

	uploaded = st.file_uploader("PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=["pdf"], accept_multiple_files=False)

	if uploaded is None:
		st.info("ì™¼ìª½ ì„¤ì •ì—ì„œ ìš”ì•½ ê¸¸ì´ë¥¼ ì„ íƒí•˜ê³ , PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
		return

	# Read bytes first (works for large files as well; Streamlit provides a buffer)
	file_bytes = uploaded.read()
	with st.spinner("PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
		text = extract_text_from_pdf(file_bytes)

	if not text or len(text.strip()) == 0:
		st.error("í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”ë³¸(PDF ì´ë¯¸ì§€)ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFë¥¼ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.")
		return

	# ì–¸ì–´ ê°ì§€
	detected_lang = detect_language(text)
	if detected_lang == "en":
		st.info("ì˜ì–´ ë¬¸ì„œë¡œ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í•œêµ­ì–´ ë²ˆì—­ íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
		if st.button("í•œê¸€ ë²ˆì—­ íŒŒì¼ ìƒì„±"):
			with st.spinner("ì˜â†’í•œ ë²ˆì—­ ì¤‘..."):
				translated = translate_to_korean(text)
			st.success("ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì €ì¥í•˜ì„¸ìš”.")
			st.download_button(
				"ë²ˆì—­ í…ìŠ¤íŠ¸ ì €ì¥ (TXT)",
				data=translated.encode("utf-8"),
				file_name="translated_ko.txt",
				mime="text/plain",
			)

	orig_chars, orig_words, orig_sents = format_stats(text)
	st.subheader("ì›ë¬¸ í†µê³„")
	col1, col2, col3 = st.columns(3)
	col1.metric("ë¬¸ì ìˆ˜", f"{orig_chars:,}")
	col2.metric("ë‹¨ì–´ ìˆ˜(ëŒ€ëµ)", f"{orig_words:,}")
	col3.metric("ë¬¸ì¥ ìˆ˜(ëŒ€ëµ)", f"{orig_sents:,}")

	if st.button("ìš”ì•½í•˜ê¸°", type="primary"):
		with st.spinner("ìš”ì•½ ìƒì„± ì¤‘..."):
			summary = summarize_text(text, max_sentences=max_sentences)
		if not summary:
			st.warning("ìš”ì•½ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìš”ì•½ ë¬¸ì¥ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.")
			return

		st.subheader("ìš”ì•½")
		st.text_area("ìš”ì•½ ê²°ê³¼", value=summary, height=240)
		st.download_button("ìš”ì•½ ì €ì¥ (TXT)", data=summary.encode("utf-8"), file_name="summary.txt", mime="text/plain")

		sum_chars, sum_words, sum_sents = format_stats(summary)
		st.caption(f"ìš”ì•½ í†µê³„ â€” ë¬¸ì {sum_chars:,}, ë‹¨ì–´ {sum_words:,}, ë¬¸ì¥ {sum_sents:,}")


if __name__ == "__main__":
	main()


